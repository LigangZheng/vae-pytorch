{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# plotting params\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 12\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10fa7ff10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(423212)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       # transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                   ])),\n",
    "    batch_size=128, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       # transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                   ])),\n",
    "                    batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = x.data.numpy()\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = np.clip(x, 0, 1)\n",
    "    x = x.reshape([-1, 28, 28])\n",
    "    return x\n",
    "\n",
    "def plot_reconstructions(model, save=True, name=None):\n",
    "    # encode then decode\n",
    "    data, _ = next(iter(test_loader))\n",
    "    data = Variable(data.view([-1, 784]), volatile=True)\n",
    "    true_imgs = data\n",
    "    encoded_imgs = F.relu(model.encoder(data))\n",
    "    decoded_imgs = model.decoder(encoded_imgs)\n",
    "    \n",
    "    true_imgs = to_img(true_imgs)\n",
    "    decoded_imgs = to_img(decoded_imgs)\n",
    "    \n",
    "    n = 10\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(true_imgs[i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(decoded_imgs[i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    if save:\n",
    "        plt.savefig('./plots/' + name + '.png', format='png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Auto-Encoder\n",
    "\n",
    "We'll start with the simplest autoencoder: a single, fully-connected layer as the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, encoding_dim)\n",
    "        self.decoder = nn.Linear(encoding_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = F.relu(self.encoder(x))\n",
    "        decoded = F.tanh(self.decoder(encoded))\n",
    "        return decoded, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "encoding_dim = 32\n",
    "\n",
    "model = AutoEncoder(input_dim, encoding_dim)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l1_penalty(var):\n",
    "    return torch.abs(var).sum()\n",
    "\n",
    "def train(epoch, l1_weight=1e-5):\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data.view([-1, 784]))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        decoder_out, encoder_out = model(data)\n",
    "        mse_loss = F.mse_loss(decoder_out, data)\n",
    "        l1_reg = l1_weight * l1_penalty(encoder_out)\n",
    "        loss = mse_loss + l1_reg\n",
    "\n",
    "        # output = model(data)\n",
    "        # loss = F.mse_loss(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(1,  num_epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_reconstructions(model, save=True, name='simple_l1_regularization_tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(DeepAutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(64, encoding_dim), \n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True), \n",
    "            nn.Linear(128, input_dim),\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "encoding_dim = 32\n",
    "\n",
    "model = DeepAutoEncoder(input_dim, encoding_dim)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data.view([-1, 784]))\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.mse_loss(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.928460\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.257657\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.268432\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.233613\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.188885\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.182713\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.156641\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.149665\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.154533\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.133467\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.135938\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.133168\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.122831\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.120312\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.121331\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.112132\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.119316\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.115723\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.122426\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.110346\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.113222\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.103152\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.105297\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.104959\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.101272\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.100843\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.104108\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.105965\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.106771\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.085396\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.097309\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.092915\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.097486\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.093793\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.098925\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.097160\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.091071\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.095100\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.086596\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.093238\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.087051\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.089791\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.088503\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.083983\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.084352\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.089679\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.086177\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.084073\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.086605\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.087268\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.083534\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.082645\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.082718\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.087828\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.082972\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.084674\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.083434\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.083450\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.083845\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.080967\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.080206\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.081843\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.080162\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.076215\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.080224\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.080452\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.080078\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.080080\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.080377\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.078188\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.079385\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.078310\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.079246\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.081053\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.077015\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.075248\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.080178\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.081267\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.073477\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.073295\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.074710\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.076231\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.076584\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.073977\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.076500\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.072496\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.077442\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.078003\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.073721\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.074713\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.077181\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.066016\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.072941\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.078106\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.074179\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.074362\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.075747\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.075110\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.072164\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.070580\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.070848\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.070065\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.073490\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.071898\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.080800\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.071516\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.071426\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.075152\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.073854\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.073381\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.072876\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.072637\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.076713\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.072249\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.073278\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.073486\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.072843\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.065426\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.070027\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.069082\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.075743\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.068733\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.074995\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.068493\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.074485\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.072806\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.074872\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.072572\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.069021\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.069021\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.071588\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.070643\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.069271\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.071051\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.076604\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.070736\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.069835\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.070133\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.072232\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.073741\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.070184\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.069363\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.070884\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.072989\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.074105\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.068299\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.070519\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.073449\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.071058\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.067938\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.067971\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.074604\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.072311\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.069102\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.069072\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.068565\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.068552\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.069866\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.069298\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.068712\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.065577\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.066857\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.068218\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.071172\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.065286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.067348\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.073046\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.070766\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.074921\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.071422\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.064597\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.068898\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.071140\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.069984\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.071232\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.068053\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.066481\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.067127\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.070119\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.069764\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.066712\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.068380\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.062333\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.068619\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.069956\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.065929\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.064813\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 0.068694\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.061695\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.069352\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.065647\n",
      "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 0.067702\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.070829\n",
      "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 0.064618\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.071681\n",
      "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 0.069079\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.068092\n",
      "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 0.069697\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.068747\n",
      "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 0.067674\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.069056\n",
      "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 0.062686\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 0.066210\n",
      "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 0.064685\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 0.070050\n",
      "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 0.063830\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 0.069115\n",
      "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 0.065400\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 0.071554\n",
      "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 0.068945\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.063497\n",
      "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 0.064059\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 0.060954\n",
      "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 0.062062\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 0.062826\n",
      "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 0.064027\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 0.063387\n",
      "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 0.068727\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 0.056992\n",
      "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 0.067273\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.070193\n",
      "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 0.064232\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 0.067483\n",
      "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 0.063804\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 0.066257\n",
      "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 0.066545\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 0.064403\n",
      "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 0.064230\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 0.063117\n",
      "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 0.065636\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.064588\n",
      "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 0.062420\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 0.062059\n",
      "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 0.064935\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 0.064866\n",
      "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 0.063089\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 0.065455\n",
      "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 0.066039\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(1,  num_epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_reconstructions(model, save=True, name=None):\n",
    "    # encode and decode the test set\n",
    "    data, _ = next(iter(test_loader))\n",
    "    data = Variable(data.view([-1, 784]), volatile=True)\n",
    "    true_imgs = data\n",
    "    encoded_imgs = F.relu(model.encoder(data))\n",
    "    decoded_imgs = model.decoder(encoded_imgs)\n",
    "    \n",
    "    true_imgs = to_img(true_imgs)\n",
    "    decoded_imgs = to_img(decoded_imgs)\n",
    "    \n",
    "    n = 10\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(true_imgs[i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(decoded_imgs[i])\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    if save:\n",
    "        plt.savefig('./plots/' + name + '.png', format='png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstructions(model, save=True, name='stacked_ae_tanh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
